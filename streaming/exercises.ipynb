{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"94ce5725-ea04-4cde-9ef3-d86968f2bd2a","showTitle":false,"title":""}},"outputs":[],"source":["BASE_PATH = \"teste-1\"\n","EXERCISE_NAME = \"workshop-exercise-1\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"a9c58afd-6623-4d51-a7f0-d77a4a6adcd6","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","import time\n","import random\n","\n","\n","# Define the schema for the generated events\n","schema = StructType([\n","    StructField(\"order_id\", IntegerType(), True),\n","    StructField(\"merchant_id\", StringType(), True),\n","    StructField(\"order_value\", FloatType(), True),\n","    StructField(\"timestamp\", TimestampType(), True),  # Add timestamp to the schema\n","])\n","\n","# Define a generator function to create random events\n","def generate_merchant_id():\n","    # Update the choices for you favorite restaurants\n","    return random.choice([\"A\", \"B\", \"C\"])\n","\n","def generate_order_value():\n","    return float(random.uniform(10, 100))\n","\n","# Register the UDFs to be used with the streaming DataFrame\n","generate_merchant_id_udf = udf(generate_merchant_id, StringType())\n","generate_order_value_udf = udf(generate_order_value, FloatType())\n","\n","spark.udf.register(\"generate_merchant_id\", generate_merchant_id, StringType())\n","spark.udf.register(\"generate_order_value\", generate_order_value, FloatType())\n","\n","# Create a streaming DataFrame using the generator and schema\n","streaming_df = spark \\\n","    .readStream \\\n","    .format(\"rate\") \\\n","    .option(\"rowsPerSecond\", 1) \\\n","    .load() \\\n","    .selectExpr(\"value as order_id\")\n","\n","# Apply the UDFs to generate the event data\n","streaming_df = streaming_df \\\n","    .withColumn(\"merchant_id\", generate_merchant_id_udf()) \\\n","    .withColumn(\"order_value\", generate_order_value_udf()) \\\n","    .withColumn(\"timestamp\", current_timestamp())  # Add current timestamp\n","\n","# Write the streaming DataFrame to a Delta table\n","query = streaming_df \\\n","    .writeStream \\\n","    .outputMode(\"append\") \\\n","    .format(\"delta\") \\\n","    .option(\"checkpointLocation\", f\"/{BASE_PATH}/{EXERCISE_NAME}/checkpoint\") \\\n","    .option(\"path\", f\"{BASE_PATH}/{EXERCISE_NAME}/data\") \\\n","    .start()\n","\n","# Wait for the stream to end\n","# query.awaitTermination()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"b1d26814-4720-488a-8158-c92d7d803d58","showTitle":false,"title":""}},"outputs":[],"source":["df_events = spark.read.format(\"delta\").load(f\"/{BASE_PATH}/{EXERCISE_NAME}/data\")\n","\n","display(df_events)"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"34fc4e45-ded4-4b63-8cfd-a7e6170b519c","showTitle":false,"title":""}},"source":["### Exercise 1: Read Table Using Streaming and use group by function\n","In this exercise, you will learn how to read a streaming Delta table in PySpark and perform a windowing aggregation. Specifically, you will count the number of orders for each merchant and also get the average ticket for each merchant in the dataset.\n","\n","Here is your task: Count the number of orders for each merchant and the average ticket, using the \"merchant_id\" field as the key. Use the groupBy function in PySpark to do this."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f392e49d-e43b-41bb-8992-41c4bf440a9b","showTitle":false,"title":""}},"outputs":[],"source":["# Your code goes here "]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"df325ccd-5efa-4141-baef-1ce3a047351a","showTitle":false,"title":""}},"source":["### Exercise 2: Windowing with Watermark\n","In the second exercise, you will be using a technique known as \"windowing with watermark\".\n","\n","Your task: Read the same Delta table, group the data by \"merchant_id\" and count the number of orders in each window of time, say 1 minute and with a step of 10 seconds. Use a watermark of 10 seconds to allow late data."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9e94fb35-3872-41db-84d6-b2768d703f80","showTitle":false,"title":""}},"outputs":[],"source":["# Your code goes here "]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"28109c84-2781-469c-a548-886ba19d06bc","showTitle":false,"title":""}},"source":["Exercise 3: Using Arbitrary Statefull Aggregation\n","\n","In this exercise, you will be using arbitrary stateful processing. This allows you to perform more complex calculations over the data stream.\n","\n","Your task: Implement the same aggregation as in the previous exercises, but this time using arbitrary stateful processing.\n","\n","\n","> Remember, the flatMapGroupWithState only works in `Scala`."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"7aa0e78f-efbf-4189-80f2-e1cd3a5ce4f4","showTitle":false,"title":""}},"outputs":[],"source":["%scala\n","\n","// your code goes here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9da10dbe-0a9c-4586-8c4d-966d9b0c52d2","showTitle":false,"title":""}},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"teste","widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
