{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69fb0c99-f9fe-4bac-bb38-defd1b020995",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f8fc201-aa41-4015-b731-bb3e7e4b3017",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "# Define the schema for the generated events\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"words\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "# Define a generator function to create random words\n",
    "def generate_random_word():\n",
    "    words = [\"apple\", \"BaNana\", \"banana\",\"cherry\", \"mango\", \"inova\" ,\"INOVA\", \"iFood\"]\n",
    "    return random.choice(words)\n",
    "\n",
    "# Register the UDF to be used with the streaming DataFrame\n",
    "generate_random_word_udf = udf(generate_random_word, StringType())\n",
    "spark.udf.register(\"generate_random_word\", generate_random_word, StringType())\n",
    "\n",
    "# Create a streaming DataFrame using the generator and schema\n",
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 1) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"value as id\")\n",
    "\n",
    "# Apply the UDF to generate the random word\n",
    "streaming_df = streaming_df \\\n",
    "    .withColumn(\"random_word\", generate_random_word_udf()) \\\n",
    "    .withColumn(\"timestamp\", current_timestamp())  # Add current timestamp\n",
    "\n",
    "# Write the streaming DataFrame to a Delta table\n",
    "query = streaming_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{BUCKET_NAME}/workshop-exemple-word-count/data-generator/checkpoint\") \\\n",
    "    .option(\"path\", f\"{BUCKET_NAME}/workshop-exemple-word-count/data-generator/data\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the stream to end\n",
    "# query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb388a21-380e-4aef-9dff-dbb9beffb1f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_events = spark.read.format(\"delta\").load(f\"{BUCKET_NAME}/workshop-exemple-word-count/data-generator/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ed728b6-8b46-40dc-814d-5412b8a4a65d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42ce18ce-70e1-481f-a184-c310e4914571",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the Delta table as a streaming DataFrame\n",
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(f\"{BUCKET_NAME}/workshop-exemple-word-count/data-generator/data\")  # Replace with your actual path\n",
    "\n",
    "# Group by the \"random_word\" column and count the occurrences of each word\n",
    "word_counts = streaming_df.groupBy(\"random_word\").count()\n",
    "\n",
    "# Write the streaming word counts to the console\n",
    "# query = word_counts \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .option(\"checkpointLocation\", f\"{BUCKET_NAME}/workshop-exemple-word-count/word-count-simples/checkpoint\") \\\n",
    "#     .start()\n",
    "\n",
    "display(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c841df8-6069-4c43-a083-51d235358957",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "# Read the Delta table as a streaming DataFrame\n",
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(f\"{BUCKET_NAME}/workshop-exemple-word-count/data-generator/data\")  # Replace with your actual path\n",
    "\n",
    "# Group by the \"random_word\" column and count the occurrences of each word\n",
    "word_counts = (\n",
    "  streaming_df\n",
    "  .withColumn(\"random_word_lowercase\", F.lower(\"random_word\"))\n",
    "  .groupBy(\"random_word_lowercase\").count()\n",
    ")\n",
    "\n",
    "# Write the streaming word counts to the console\n",
    "# query = word_counts \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .option(\"checkpointLocation\", f\"{BUCKET_NAME}/workshop-exemple-word-count/word-count-lower-case/checkpoint\") \\\n",
    "#     .start()\n",
    "\n",
    "display(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "711d54fb-86cc-4e98-a761-76a7ca8ce4b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "# Read the Delta table as a streaming DataFrame\n",
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(f\"{BUCKET_NAME}/workshop-exemple-word-count/data-generator/data\")  # Replace with your actual path\n",
    "\n",
    "# Group by the \"random_word\" column and count the occurrences of each word\n",
    "word_counts = (\n",
    "  streaming_df\n",
    "  .withColumn(\"random_word_lowercase\", F.lower(\"random_word\"))\n",
    "  .groupBy(\"random_word_lowercase\").count()\n",
    ")\n",
    "\n",
    "# Write the streaming word counts to the console\n",
    "# query = word_counts \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .option(\"checkpointLocation\", f\"{BUCKET_NAME}/workshop-exemple-word-count/word-count-lower-case/checkpoint\") \\\n",
    "#     .start()\n",
    "\n",
    "display(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3bebfd1-dfde-4432-b8bd-0ca31f00445d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "\n",
    "val BUCKET_NAME = \"path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8452605-ba56-4037-94e7-21ab423617fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "\n",
    "\n",
    "import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode}\n",
    "import org.apache.spark.sql.{Dataset, Encoders, Row, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming.GroupState\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import org.apache.spark.sql.catalyst.encoders.RowEncoder\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// Define the WordCount case class\n",
    "  val OUTPUT_SCHEMA: StructType = new StructType()\n",
    "    .add(\"word\", StringType)\n",
    "    .add(\"count\", LongType)\n",
    "\n",
    "    \n",
    "// Define the mapping function\n",
    "def flatMapGroupsWithStateFunction\n",
    "(\n",
    "  id: String,\n",
    "  iterator: Iterator[Row],\n",
    "  state: GroupState[Map[String, Long]]\n",
    "): Iterator[Row] = {\n",
    "  // FIXME: We are only considering 1x for each microbatch. We should iterate over all rows and process each of them.\n",
    "  var wordCount = state.getOption.getOrElse(Map())\n",
    "\n",
    "  val updatedCount:Long = wordCount.get(id).map(_ + 1).getOrElse(1)\n",
    "  val wordCountUpdated: Map[String, Long] = wordCount + (id -> updatedCount)\n",
    "\n",
    "  state.update(wordCountUpdated)\n",
    "  val rowIterator: Iterator[Row] = wordCountUpdated.map { case (key, value) => Row(key, value.toLong) }.iterator\n",
    "\n",
    "  rowIterator\n",
    "}\n",
    "\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._\n",
    "\n",
    "// Read the Delta table as a streaming DataFrame\n",
    "val df = spark.readStream.format(\"delta\").load(s\"${BUCKET_NAME}/workshop-exemple-word-count/data-generator/data\")\n",
    "\n",
    "val inputColumnSchema: StructType = df.schema\n",
    "\n",
    "val groupedDataFrame = {\n",
    "  implicit val inputRowEncoder= RowEncoder(inputColumnSchema)\n",
    "\n",
    "  df\n",
    "    .as[Row]\n",
    "    .groupByKey { row: Row =>\n",
    "      row.getAs[String](\"random_word\")\n",
    "    }\n",
    "}\n",
    "\n",
    "// Split the words into separate rows\n",
    "val words = df.selectExpr(\"explode(split(random_word, ' ')) as word\")\n",
    "\n",
    "// Convert the DataFrame of words to Dataset[WordCount]\n",
    "\n",
    "\n",
    "val wordCounts = {\n",
    "  implicit val outputRowEncoder = RowEncoder(OUTPUT_SCHEMA)\n",
    "  groupedDataFrame\n",
    "          .flatMapGroupsWithState(\n",
    "            outputMode = OutputMode.Update(),\n",
    "            timeoutConf = GroupStateTimeout.ProcessingTimeTimeout()\n",
    "  )(\n",
    "    flatMapGroupsWithStateFunction\n",
    "  )\n",
    "  .toDF()\n",
    "}\n",
    "\n",
    "def saving(dataFrame: DataFrame, batchId: Long): Unit = {\n",
    "  dataFrame\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(s\"${BUCKET_NAME}/workshop-exemple-word-count-arbirtary-agg/data\")\n",
    "}\n",
    "wordCounts\n",
    "          .writeStream\n",
    "          .queryName(\"query_nrt\")\n",
    "          .option(\"checkpointLocation\",f\"${BUCKET_NAME}/workshop-exemple-word-count-arbirtary-agg/checkpoint\")\n",
    "          .format(\"delta\")\n",
    "          .outputMode(\"update\")\n",
    "          /*\n",
    "           * This is a required trick. Because it uses flatMapGroupsWithState in update mode, we also need to use update mode here.\n",
    "           * However, we will not be able to write using update mode (\"... DeltaDataSource does not support Update output mode\").\n",
    "           * So, to overcome it, we use a foreachBath and, inside it, we perform some appends.\n",
    "           *\n",
    "           * Due to some changes in Scala 2.12, the method DataStreamWriter.foreachBatch requires some updates on the code,\n",
    "           * otherwise this ambiguity happens. Because of that we create a new method to pass to foreachBatch instead\n",
    "           * the writer the code inside it.\n",
    "           * Source: https://issues.apache.org/jira/browse/SPARK-26132?focusedCommentId=17178019&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17178019\n",
    "           */\n",
    "          .foreachBatch(saving _)\n",
    "          .trigger(Trigger.ProcessingTime(5))\n",
    "          .start()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbc13918-8578-4e8b-9907-7584c762a987",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.format(\"delta\").load(f\"{BUCKET_NAME}/workshop-exemple-word-count-arbirtary-agg/data\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fbb76bd-d1f1-47c2-a804-bfb596c64bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Workshop Inova - Examples",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
